{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First exercise: count the number of lines in Python for each file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "nb lines in filedownloads/bookings.csv : 10000011\n"
     ]
    }
   ],
   "source": [
    "def countLines(fileName:String) : Unit = {\n",
    "    val bookings_rdd = sc.textFile(fileName)\n",
    "    //val nb_lines = bookings_rdd.map(line => 1).reduce(_+_)\n",
    "    println(\"nb lines in file\" + fileName + \" : \" + bookings_rdd.count())\n",
    "    }\n",
    "    \n",
    "//TODO bz2 files\n",
    "countLines(\"downloads/bookings.csv\")\n",
    "//countLines(\"downloads/archives.csv.bz2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second exercise: top 10 arrival airports in the world in 2013 (using the bookings file)**\n",
    "\n",
    "Arrival airport is the column arr_port. It is the IATA code for the airport\n",
    "\n",
    "To get the total number of passengers for an airport, you can sum the column pax, grouping by arr_port. Note that there is negative pax. That corresponds to cancelations. So to get the total number of passengers that have actually booked, you should sum including the negatives (that will remove the canceled bookings).\n",
    "\n",
    "Print the top 10 arrival airports in the standard output, including the number of passengers.\n",
    "\n",
    "Bonus point: Get the name of the city or airport corresponding to that airport (programatically, we suggest to have a look at GeoBases in Github)\n",
    "\n",
    "Bonus point: Solve this problem using pandas (instead of any other approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 63 in stage 16.0 failed 1 times, most recent failure: Lost task 63.0 in stage 16.0 (TID 609, localhost): java.lang.NumberFormatException: null\n",
       "\tat java.lang.Integer.parseInt(Integer.java:542)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike$class.toInt(StringLike.scala:229)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:31)\n",
       "\tat com.databricks.spark.csv.util.TypeCast$.castTo(TypeCast.scala:61)\n",
       "\tat com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$6.apply(CsvRelation.scala:194)\n",
       "\tat com.databricks.spark.csv.CsvRelation$$anonfun$buildScan$6.apply(CsvRelation.scala:173)\n",
       "\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)\n",
       "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:511)\n",
       "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:686)\n",
       "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95)\n",
       "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)\n",
       "org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)\n",
       "org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1397)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1384)\n",
       "org.apache.spark.sql.execution.TakeOrderedAndProject.collectData(basicOperators.scala:213)\n",
       "org.apache.spark.sql.execution.TakeOrderedAndProject.executeCollect(basicOperators.scala:218)\n",
       "org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n",
       "org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n",
       "org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n",
       "org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n",
       "org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)\n",
       "org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498)\n",
       "org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505)\n",
       "org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1375)\n",
       "org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1374)\n",
       "org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099)\n",
       "org.apache.spark.sql.DataFrame.head(DataFrame.scala:1374)\n",
       "org.apache.spark.sql.DataFrame.take(DataFrame.scala:1456)\n",
       "org.apache.spark.sql.DataFrame.showString(DataFrame.scala:170)\n",
       "org.apache.spark.sql.DataFrame.show(DataFrame.scala:350)\n",
       "org.apache.spark.sql.DataFrame.show(DataFrame.scala:311)\n",
       "$line35.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.topAirportsSQL(<console>:58)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:39)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:44)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:46)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:48)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:50)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:52)\n",
       "$line36.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:54)\n",
       "$line36.$read$$iwC$$iwC$$iwC.<init>(<console>:56)\n",
       "$line36.$read$$iwC$$iwC.<init>(<console>:58)\n",
       "$line36.$read$$iwC.<init>(<console>:60)\n",
       "$line36.$read.<init>(<console>:62)\n",
       "$line36.$read$.<init>(<console>:66)\n",
       "$line36.$read$.<clinit>(<console>)\n",
       "$line36.$eval$.<init>(<console>:7)\n",
       "$line36.$eval$.<clinit>(<console>)\n",
       "$line36.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:497)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SQLContext, Row, DataFrame}\n",
    "\n",
    "def topAirportsSQL(fileName:String, nb_top:Int) : Unit = {    \n",
    "    val sqlContext = new org.apache.spark.sql.SQLContext(sc)  \n",
    "    import sqlContext.implicits._\n",
    "    var bookings_df = sqlContext.load(\"com.databricks.spark.csv\", Map(\"path\" -> fileName,\n",
    "                                                                      \"header\"->\"true\", \n",
    "                                                                      \"charset\"->\"UTF8\",\n",
    "                                                                      \"delimiter\"->\"^\", \n",
    "                                                                      \"nullvalue\" -> \"NULL\",\n",
    "                                                                      \"inferschema\" -> \"true\",\n",
    "                                                                      \"treatEmptyValuesAsNulls\" -> \"true\",\n",
    "                                                                      \"parserLib\" -> \"univocity\",\n",
    "                                                                      \"ignoreTrailingWhiteSpace\" ->\"true\",\n",
    "                                                                      \"codec\" ->\"bzip2\"\n",
    "                                                                     ))                                            \n",
    "   // bookings_df.printSchema()\n",
    "    val topAirports = bookings_df.select(\"arr_port\", \"pax\")\n",
    "    .na.fill(0)\n",
    "                                 .groupBy(\"arr_port\")\n",
    "                                 .sum()\n",
    "                                 .sort($\"sum(pax)\".desc)\n",
    "\n",
    "\n",
    "    topAirports.show(nb_top)\n",
    "    }\n",
    "\n",
    "topAirportsSQL(\"downloads/bookings.csv\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Third exercise: plot the monthly number of searches for flights arriving at Málaga, Madrid or Barcelona**\n",
    "\n",
    "For the arriving airport, you can use the Destination column in the searches file. Plot a curve for Málaga, another one for Madrid, and another one for Barcelona, in the same figure. Bonus point: Solving this problem using pandas (instead of any other approach)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
